<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Language-Guided World Models: Enhancing Human Control over Artificial Agents">
  <meta property="og:title" content="Language-Guided World Models: Enhancing Human Control over Artificial Agents" />
  <meta property="og:description" content="Language-Guided World Models: Enhancing Human Control over Artificial Agents" />
  <meta property="og:url" content="https://language-guided-world-model.github.io" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Language-Guided World Models: Enhancing Human Control over Artificial Agents">
  <meta name="twitter:description" content="Language-Guided World Models: Enhancing Human Control over Artificial Agents">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="ai, world model, language">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Language-Guided World Models: A Model-Based Approach to AI Control</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Language-Guided World Models <img src="static/images/icon.png" alt="language-guided world model" style="width:4rem; height:4rem;"><br> A Model-Based Approach to AI Control</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://alexzhang13.github.io" target="_blank">Alex Zhang</a><sup>*</sup><sup>&dagger;</sup>,</span>
              <span class="author-block">
                <a href="https://machineslearner.com" target="_blank">Khanh Nguyen</a><sup>*</sup><sup>&Dagger;</sup>,</span>
              <span class="author-block">
                <a href="https://jens321.github.io" target="_blank">Jens Tuyls</a><sup>&dagger;</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/albertkuilin/" target="_blank">Albert Lin</a><sup>&dagger;</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.princeton.edu/~karthikn/" target="_blank">Karthik Narasimhan</a><sup>&dagger;</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Princeton University<sup>&dagger;</sup> & UC Berkeley<sup>&Dagger;</sup></span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body has-text-centered">
        <h2 class="subtitle">
        <b>We develop world models that can be adapted with natural language. Intergrating these models into artificial agents allows humans to effectively control these agents through verbal communication.</b>                </h2>
        <!-- Your video here -->
        <div class="has-text-centered">
          <img src="static/images/application.png" alt="teaser" style="max-width: 50%">
        </div>
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4"> -->
         <h2 class="subtitle">
         The above image illustrates the type of safe and transparent human-AI communication that our world models enable. Instead of executing a task immediately, an agent uses its world model to generate a plan for a human to review. This helps the human better understand the agent's intention. The human can also improve the plan by correcting the agent's actions or adapting its world model through language.
        </h2>

      </div>
    </div>
  </section>
  <!-- End teaser video --> 



  <!-- Paper abstract -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p style="font-weight: normal">
              Putting artificial agents under human control is vital in order to leverage their powerful capabilities. Installing probabilistic world models into artificial agents opens an efficient channel for humans to control these agents.  In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world
models are difficult for humans to adapt because they lack a natural communication
interface.  
<br/><br/>
We develop <strong>Language-Guided World Models</strong> (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. 
<br/><br/>
To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER  (<a href="https://arxiv.org/abs/2101.07393">Hanjie et al.,2021</a>), requiring compositional generalization to new language descriptions and environment dynamics. Our experiments reveal that the current state-of-the-art
Transformer architecture performs poorly on this benchmark, motivating us to design a more robust architecture.  To showcase the practicality of our proposed LWMs, we simulate a scenario where these models augment the interpretability and safety of an agent by enabling it to generate and discuss plans with a human before execution. By effectively incorporating language feedback on the plan, the models boost the agent performance in the real environment by up to three times without collecting any interactive experiences in this environment
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- How it Works -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Our Approach</h2>
          <div class="content has-text-justified">
            <p style="font-weight: normal">
                Learning LWMs poses a challenging problem involving the retrieval and incorporation of information expressed in different modalities. Our model is an encoder-decoder Transformer which encodes a manual and decodes a trajectory. We transform the trajectory into a long sequence of tokens and train the model as a sequence generator. We implement a specialized attention mechanism inspired by EMMA (<a href="https://arxiv.org/abs/2101.07393">Hanjie et al., 2021</a>) to incorporate textual information into the observation tokens.
                <br> <br>
                <img src="static/images/model.png" alt="model" style="max-width: 100%">
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- End youtube video -->

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-three-fifths has-text-centered">
            <!-- Paper video. -->
            <h2 class="title is-3">Benchmarking Compositional Generalizability</h2>
            <div class="content is-centered has-text-justified">
              <p style="font-weight: normal">
              Our goal is to build world models that can generalize to <i>compositionally</i> novel texts and environment dynamics. We construct a challenging benchmark based on the MESSENGER environment to evaluate this capability of world models. In MESSENGER, a player manages to pick up a <i>message</i> entity and deliver to a <i>goal</i> entity without colliding with an <i>enemy</i> entity. There is a manual describing the identity and attributes of the entities. A model is tested on previously unseen environments that are increasingly dissimilar to the training environments.
              </p>
              <br>
              <div class="has-text-centered">
                <img src="static/images/messenger.png" alt="messenger" width="400">
              </div>
             </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-three-fifths has-text-centered">
            <!-- Paper video. -->
            <h2 class="title is-3">Results</h2>
            <div class="content is-centered has-text-justified">
              <p style="font-weight: normal">
              We demonstrate the effectiveness of our proposed model through both intrinsic and extrinsic evaluations. The instrinsic evaluation measures the prediction loss of the model when conditioned on ground-truth observations (we also have results with self-generated trajectories in the paper). The extrinsic evaluation simulates the scenario we describe at the top of this page, in which an agent learns from a human using its world model, without interacting with the real environment. In both evaluations, our model outperforms the standard encoder-decoder Transformer and approaches the performance of an oracle with a perfect semantic-parsing capability. 
              </p>
              <div class="has-text-centered">
                <img src="static/images/result_intrinsic.png" alt="result intrinsic" style="max-width: 49%">
                <img src="static/images/result_extrinsic.png" alt="result extrinsic" style="max-width: 49%">
              </div>
              <p>Below, we show a qualitative example taken from our hardest evaluation setting. The Observational (no language) model mistakenly captures the movement patterns of the immobile queen goal and the chasing whale message. It also misrecognizes the whale as an enemy, predicting a wrong reward after the player collides with this entity. GPTHard is an approach that levarages ChatGPT to ground descriptions to entities. It falsely identifies the queen as the message and predicts the whale to be fleeing. Meanwhile, our model (EMMA) captures all of those roles and movements accurately.</p>
              <div class="has-text-centered">
                <img src="static/images/teaser.gif" alt="result intrinsic" style="max-width: 100%">
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre>
<code>@article{zhang2024languageguided
  title={Language-Guided World Models: Shaping Artificial Minds through Words},
  author={Zhang, Alex and Nguyen, Khanh and Tuyls, Jens and Lin, Albert and Narasimhan, Karthik},
  year={2024},
  journal={arXiv},
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
